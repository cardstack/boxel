{
  "data": {
    "type": "card",
    "attributes": {
      "modelId": "mistralai/ministral-8b",
      "canonicalSlug": "mistralai/ministral-8b",
      "name": "Mistral: Ministral 8B",
      "created": 1729123200,
      "description": "Ministral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.",
      "pricing": {
        "prompt": "0.0000001",
        "completion": "0.0000001",
        "request": "0",
        "image": "0"
      },
      "contextLength": 131072,
      "architecture": {
        "modality": "text->text",
        "inputModalities": [
          "text"
        ],
        "outputModalities": [
          "text"
        ],
        "tokenizer": "Mistral",
        "instructType": ""
      },
      "topProvider": {
        "isModerated": false,
        "contextLength": 131072,
        "maxCompletionTokens": null
      },
      "perRequestLimits": null,
      "supportedParameters": [
        "frequency_penalty",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_p"
      ],
      "defaultParameters": {
        "temperature": 0.3
      },
      "cardInfo": {
        "title": null,
        "description": null,
        "thumbnailURL": null,
        "notes": null
      }
    },
    "meta": {
      "adoptsFrom": {
        "module": "../openrouter-model",
        "name": "OpenRouterModel"
      }
    }
  }
}