{
  "data": {
    "type": "card",
    "attributes": {
      "modelId": "nvidia/llama-3.3-nemotron-super-49b-v1.5",
      "canonicalSlug": "nvidia/llama-3.3-nemotron-super-49b-v1.5",
      "name": "NVIDIA: Llama 3.3 Nemotron Super 49B V1.5",
      "created": 1760101395,
      "description": "Llama-3.3-Nemotron-Super-49B-v1.5 is a 49B-parameter, English-centric reasoning/chat model derived from Meta’s Llama-3.3-70B-Instruct with a 128K context. It’s post-trained for agentic workflows (RAG, tool calling) via SFT across math, code, science, and multi-turn chat, followed by multiple RL stages; Reward-aware Preference Optimization (RPO) for alignment, RL with Verifiable Rewards (RLVR) for step-wise reasoning, and iterative DPO to refine tool-use behavior. A distillation-driven Neural Architecture Search (“Puzzle”) replaces some attention blocks and varies FFN widths to shrink memory footprint and improve throughput, enabling single-GPU (H100/H200) deployment while preserving instruction following and CoT quality.\n\nIn internal evaluations (NeMo-Skills, up to 16 runs, temp = 0.6, top_p = 0.95), the model reports strong reasoning/coding results, e.g., MATH500 pass@1 = 97.4, AIME-2024 = 87.5, AIME-2025 = 82.71, GPQA = 71.97, LiveCodeBench (24.10–25.02) = 73.58, and MMLU-Pro (CoT) = 79.53. The model targets practical inference efficiency (high tokens/s, reduced VRAM) with Transformers/vLLM support and explicit “reasoning on/off” modes (chat-first defaults, greedy recommended when disabled). Suitable for building agents, assistants, and long-context retrieval systems where balanced accuracy-to-cost and reliable tool use matter.\n",
      "pricing": {
        "prompt": "0.0000001",
        "completion": "0.0000004",
        "request": "0",
        "image": "0"
      },
      "contextLength": 131072,
      "architecture": {
        "modality": "text->text",
        "inputModalities": [
          "text"
        ],
        "outputModalities": [
          "text"
        ],
        "tokenizer": "Llama3",
        "instructType": ""
      },
      "topProvider": {
        "isModerated": false,
        "contextLength": 131072,
        "maxCompletionTokens": null
      },
      "perRequestLimits": null,
      "supportedParameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "defaultParameters": null,
      "cardInfo": {
        "title": null,
        "description": null,
        "thumbnailURL": null,
        "notes": null
      }
    },
    "meta": {
      "adoptsFrom": {
        "module": "../openrouter-model",
        "name": "OpenRouterModel"
      }
    }
  }
}